#+TITLE: Examination of DDPM and CEM on Bayesian Inversion problems.
#+setupfile: ~/.emacs.d/setupfile.org

* Log
** 2024-03-15 :meeting:
- Professor Wang's server is downloading the Mayo training data.
- For this problem, CEM has to be conditioned on three parameters (instead of two), namely:
  1) Gaussian noise level $t$ (applied to ground truth $\bm{X}_0$)
  2) forward diffused ground truth $\bm{X}_t$ at noise level $t$
  3) low dose CT image
- The low does CT image is a new parameter and conditions the neural network to denoise $\bm{X}_t$ to $\widetilde{\bm{X}_0}$ with similar probability distribution to $\bm{X}_0$ (identical to earlier DDPM/CEM experiments/exercise).
- The number of training samples is too little, and we will need to create more samples akin to [cite:@adler2018_deep_bayesian_inversion]. @Huiyuan to reach out to authors on code to create new samples.
- For this exercise, we can amend the earlier conditioned U-net to allow for two channels instead of one. Professor Wang has a sample pytorch code here https://github.com/wang-zhongjian/CFNO/blob/main/codes/unet.py.
- Code for this problem is stored here https://github.com/ainuyew/bayesian-inversion.

** 2024-03-16
- Found a paper and assodicated code to simulate low-dose samples at https://github.com/smuzd/LD-CT-simulation/tree/master ([cite:@zeng2015_simple_low-dose_x-ray]). The Matlab code is dependent on MIRT library (https://github.com/JeffFessler/mirt), which is not actively maintained (author has switched to Julia). The code fails to run on Octave.
- Found another github repository (https://github.com/xinario/SAGAN; https://link.springer.com/article/10.1007/s10278-018-0056-0) working on the CT scans with similar motives (to denoise low dosage CT scans). Octave does not support all the Matlab functions (e.g. function ~fanbeam~ in the image package) required by this code.
- Read a 2020 paper (https://dx.doi.org/10.1088/1361-6560/ab8953) on simulating low dose CT scans that provided a summary on advances thus far, and introduced their better models/methods.  This goes beyond earlier methods of simply introducing Poisson noise (for quantum noise) and Gaussian noise (for electronic noise). The reearch tests various models to account for negative effects such as beam hardening, and compares their models against actual low dose CT scans.
- Alternatively ([cite:@yi2018_sharpness-aware_low_dose], https://arxiv.org/abs/1708.06453), there is a 850/dose deceased piglets CT scan data set with

** 2024-03-17
- https://github.com/odlgroup/odl/issues/1569 add Poisson noise to projection data
- https://github.com/odlgroup/odl/tree/25ec783954a85c2294ad5b76414f8c7c3cd2785d/odl/contrib/datasets/ct code for mayo reconstruction by authors
** 2024-03-18
- Tried unsuccessfully to install python 3.8 to work with ODL (last released in 2018). ODL will only work with numpy < 1.20. The author has contributed the Mayo data code to ODL. Downloaded source code of ODL with the intent to correct/update the code to work with 3.10 and associated dependencies.
description as a private header tag "WaterAttenuationCoefficient" (see https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4644156/). This is neccessary to calibrate the projections to water (HU=0).
- Unable to run sample ODL code on M1. 3D geometry ray transformation requires CUDA. Sample code show how to create training data.
- There's matlab code  (https://github.com/xinario/SAGAN/blob/master/poisson_noise_simulation/add_poisson_noise.m) that details how it's done in reference to the paper ([cite:@yi2018_sharpness-aware_low_dose]). The code adds noise to an image. The sinograms are created prior to adding of noise. A reconstructed image is returned by the function.

#+DOWNLOADED: screenshot @ 2024-03-18 19:25:45
[[file:20240318-192545_screenshot.png]]

- [cite:@yi2018_sharpness-aware_low_dose] uses images from few subjects (36 subjects). Hence, there's little independence between the images.
** 2024-03-19
- National Lung Screening Trial (NLST) at https://wiki.cancerimagingarchive.net/display/NLST has 11.TB of CT scans/images for 26,254 subjects.
- https://huggingface.co/docs/diffusers/v0.25.0/en/api/models/unet2d-cond has a conditional U-Net that ".. takes a noisy sample, conditional state, and a timestep and returns a sample shaped output.".
- Jax conditional Unet should be able to embed multiple channels in the data i.e. it has a shape of (n_batch, rows, columns, n_channel) = (1000, 512, 512, 2).
- Worked on code to train DDPM.
** 2024-03-20
- Training code is crashing with high memory consumption. Reducing number of images did not help. Will need to scale image down to see if that helps.
- It might be worthwhile to try to run this on another library from jax in case that is the problem. There are plenty of Unet implementations on CT scans e.g. https://medium.com/@fabio.sancinetti/u-net-convnet-for-ct-scan-segmentation-6cc0d465eed3. Unet was afterall created for modeling image segmentation problems for medical applications [cite:@ronneberger2015_u-net_convolutional_networks].
** 2024-03-21
- Successfully train DDPM using smaller images (128x128 instead of 512x512) with very small loss. Working on sampling code for DDPM. Will need to retrain DDPM on 90% data to reserve remaining 10% for testing.
- Need to investigate on how to measure success/failure. This should largely follow the paper [cite:@adler2018_deep_bayesian_inversion].
- Outstanding tasks/ideas:
  - CEM code
  - code to create more sample data (based on ODL library) from full dose Helical sinograms?
  - the condition is based on quarter dose images. do we need different dosage?
  - run model on full resolution 512x512
  - Mayo test data has quarter dose (and omits full dose). Lesion information is available. Some qualitative checks can be performed to see if samples show the lesions in a clearer manner?
** 2024-03-23
- managed to train DPPM and CEM on server. To update loss and numer of epochs.
- managed to generate 1000 samples on M1 using jax-metal. The mean sample is not ideal. The problem might be the loss function used (MSE). DDPM was trained for 20 epochs on 90% training data. The sample was based on the remaining unseen 10% data.
- unable to run odl sample code on local nvidia machine (ran out of memory). Will need to try this on the server.
** 2024-03-26
- Reading up on batch normalization. https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739, https://en.wikipedia.org/wiki/Batch_normalization, https://arxiv.org/abs/1502.03167.
- CEM sampling (100 samples) with data rescaled to -1 to 1 seem to produce good samples. Next task at hand is to figure out if it's better than the low dose sample. We can do this by comparing how well the lesion can be seen in the contrast. See [cite:@adler2018_deep_bayesian_inversion].
- Will want to consider to rescale data by z-score. mean and variance will be based on training data.
** 2024-03-29
- preprocessing CT scans https://zapaishchykova.medium.com/preprocessing-ct-in-python-2a7e8170ac9f
- open source software to read CT scans https://www.slicer.org
- summart on windowing CT scans https://kevalnagda.github.io/ct-windowing
* Problem Statement
This is an investigation into the use of diffusion models to solve the problem discussed in [cite:@adler2018_deep_bayesian_inversion]. Specifically, we use DDPM and CEM to model the problems. We want to investigate if DDPM and CEM will perform better than the reference method/model ([cite:@adler2018_deep_bayesian_inversion]) to denoise ultra low dose CT scans, and produce higher quality CT scans similar to the normal dose images.

In a CT scan of a subjectsbegy2pts, multiple 2D projections are generated for each angle of projection. Projections are created by the forward projection algorithm,and measures the attenutation (reduction in intensity) of the x-ray beam (cast no the subject towards the deector). A sinogram consist of mutiple projections stacked together. We can then reconstruct a 3D image of the subject of interest through a back projection algorithm such as filtered back projection (FBP). Filtered back projection is the industry standard to reconstruct images from sinograms because it is fast and robust. FBP applies a image de-blur filter (sharpen) to the projection (sinogram) and then back project the resulting projection to a 3D image. In back projection, we map the data in detector space to image space.



In [cite:@adler2018_deep_bayesian_inversion], we train two neural networks (GAN) to assist in the analysis of ultra low dose CT scans. An ultra low dose CT scan is simulated from a normal dose CT scan image. Projections are sampled from the normal dose image and Poisson noise is added to the projections. The ultra low dose CT image is then reconstructed using filtered back projection (FBP). The first neural network (deep posterior sampling) produces (high quality) sample images from the ultra low dose images. The second neural network (deep direct estimation) returns the mean and variance (of what the normal dose should be) given an ultra lose dose image.

[cite:@moen2021_low-dose_ct_image] provides an overview of the data provided for the grand challenge. In particular, the low dose projection (DICOM-CT-PD) are simulated from the normal dose projections by adding Poisson noise.

** Filtered Back Projection (FBP)
Found a site on filtered back projection https://howradiologyworks.com/filtered-backprojection-fbp-illustrated-guide-for-radiologic-technologists/#:~:text=Back%20projection%20is%20the%20process,Filtered%20Backprojection%20and%20Iterative%20Reconstruction. A traditional x-ray scan gives us a 2d image of a subject of interest from a view (usually in front of the patient for a chest scan). One 2d image does not provide us a very good understanding of the subject. A forward projection algorithm instead takes multiple 2d images of a the subject from multiple and different angles/views. The end results are _sinograms_. Each line/row in a sinogram corresponds to 2d image taken of the subject at particular view. A sinogram is so named because each point in the subject corresponds traces a sinusoid curve.

Given knowledge of how a forward projection works, we can reverse the process (which we call back projection) to reconstruct an image of the subject. Because the back projection (BP) is performed one view at a time, the resulting reconstructed image is a blurred image (https://www.youtube.com/watch?v=YvYIkbiRMy0). To remedy this problem, we can apply a sharpening step (or image deblurring). This image deblurring can be applied in three different manner:
1. BP projection to image then apply image de-blur
2. De-blur projection then BP to image
3. De-blur projection, BP to image, then finally de-blur again
The second option (called Filtered Back Projection or FBP) is the most common as it is fast and robust. Back projection is the process of mapping the data from the detector space to the image space, while forward projection is the process of mapping the data in the image space to the detector space.

* Data
We use data from 2016 Low Dose CT Grand Challenge (https://www.aapm.org/grandchallenge/lowdosect/#testDatasets). Training data is downloaded from box at https://aapm.app.box.com/s/eaw4jddb53keg1bptavvvd1sf4x3pe9h. (https://www.imagewisely.org/Imaging-Modalities/Computed-Tomography/Image-Reconstruction-Techniques) The data contains images reconstructed using two reconstruction kernels B30 and D45. Reconstruction kernels (also called â€œfilterâ€ or â€œalgorithmâ€) affects the image quality. There is a tradeoff between spatial resolution and noise. A smoother kernel generates images with lower noise but with reduced spatial resolution. A sharper kernel generates images with higher spatial resolution, but increases the image noise. Spatial resolution in CT is the ability to differentiate objects of different density. A high spatial resolution is important to distinguish objects that are close to one another.

Patient_Data directory contains the reconstructed images and projections (in DICOM-CT-PD format) of the CT scans.
Ancillary_Information contains detailed documentation on the file format DICOM-CT-PD (vendor neutral DICOM format), and lesion information.

Full (normal) and associate quarter (low) dose projections (.DCM) and associated reconstructed images (.IMA) are provided by Mayo clinic. The low dose projections are simulated from the normal dose projections [cite:@mccollough2017_low-dose_ct_detection]. The training data contain data from ten patients while the testing data contain data from twenty patients. For each patient and dosage level, there are ~48k projection files and 225 reconstructed images. Each 225 reconstructed images are 2D images which forms a 3D view of the patient. Mayo has provided reconstructed images using two thickness (1mm and 3mm) and two kernels (B30 and D45). Hence, for every patient, we have a total of 2 x 2 x 2 x 225 = 1800 image files.

In the Helical scan, pitch refers to the movement of the table in the z-axis relative to the height of the detector. A helical pitch of 1.0 means the table will move a distance equal to the height of the detector resulting in scans which do not overlap and do not have gaps. A helical pitch of 0.5 means the table will move a distance equal to half the height of the detector resulting in scans that overlap. This is usually done to improve spatial resolution.

We examine some full dose samples from the training images.
#+begin_src jupyter-python :session py :exports both :async yes :eval never-export
  from skimage.transform import iradon
  from pydicom import dcmread
  import matplotlib.pyplot as plt
  import numpy as np
  from pathlib import Path
  import tqdm

  import utils

  path='/Users/huiyuanchua/Documents/data/Mayo_Grand_Challenge/Patient_Data'

  ima_path=f'{path}/Training_Image_Data/3mm B30'
  ima_fd_path=f'{ima_path}/full_3mm/L067/full_3mm'

  pathlist = Path(ima_fd_path).rglob('*.IMA')
  ima_files = [ima for ima in pathlist]

  n = 3
  images=[]
  ima_batch = np.random.choice(ima_files, size=n**2, replace=False)
  fig, axs = plt.subplots(n, n, figsize=(3 *n, 3 * n), sharex=True, sharey=True)
  _ = fig.tight_layout()
  for i, ima_file in tqdm.tqdm(enumerate(ima_batch)):
      ima = dcmread(ima_file)
      image = ima.pixel_array

      # convert to HU
      hu_values = ima.RescaleSlope * image + ima.RescaleIntercept
      densities = (hu_values + 1000)/1000
      images.append(densities)
      ax = axs[i // n][i % n]
      #ax.imshow(image, cmap=plt.cm.Greys_r,)
      ax.imshow(utils.normalize_image(densities), cmap=plt.cm.Greys_r,)
  plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: 9it [00:00, 282.17it/s]
:
[[file:./.ob-jupyter/d106bffcb7948cba61b5dba49266665e75137c41.png]]
:END:


We apply forward projection on the batch to obtain their respective sinograms (Radon transform).
#+ATTR_LATEX: :options frame=single, linenos, breaklines, tabsize=2
#+begin_src jupyter-python :session py :exports both :async yes :eval never-export
  import numpy as np
  import matplotlib.pyplot as plt
  from skimage.data import shepp_logan_phantom
  from skimage.transform import radon, rescale
  import tqdm

  sinograms = []
  fig, axs = plt.subplots(n, n, figsize=(3 *n, 3 * n), sharex=True, sharey=True)
  _ = fig.tight_layout()
  for i, image in tqdm.tqdm(enumerate(images)):
      ax = axs[i // n][i % n]

      theta = np.linspace(0., 180., max(image.shape), endpoint=False)
      sinogram = radon(image, theta=theta)
      sinograms.append(sinogram)
      dx, dy = 0.5 * 180.0 / max(image.shape), 0.5 / sinogram.shape[0]
      ax.imshow(sinogram, cmap=plt.cm.Greys_r,
                 extent=(-dx, 180.0 + dx, -dy, sinogram.shape[0] + dy),
                 aspect='auto')

      ax.imshow(sinogram)
  plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: 0it [00:00, ?it/s]/Users/huiyuanchua/miniconda3/envs/venv310/lib/python3.10/site-packages/skimage/transform/radon_transform.py:75: UserWarning: Radon transform: image must be zero outside the reconstruction circle
:   warn('Radon transform: image must be zero outside the '
: 9it [00:06,  1.42it/s]
:
[[file:./.ob-jupyter/9908590d1629c61ffc91bfa8d539c672a7a6b737.png]]
:END:

We now apply filtered back projection to reconstruct the original image using the filter "ramp".
#+ATTR_LATEX: :options frame=single, linenos, breaklines, tabsize=2
#+begin_src jupyter-python :session py :exports both :async yes :eval never-export
  from skimage.transform import iradon
  from pydicom import dcmread
  import matplotlib.pyplot as plt

  import utils

  sinogram = sinograms[4]
  image = images[4]
  reconstruction_fbp = iradon(sinogram, theta=theta, filter_name='ramp')
  error = reconstruction_fbp - image
  print(f'FBP rms reconstruction error: {np.sqrt(np.mean(error**2)):.3g}')

  imkwargs = dict(vmin=-0.2, vmax=0.2)
  fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(8, 4.5), sharex=True, sharey=True)
  _ = fig.tight_layout()
  ax1.set_title("Ground Truth")
  ax1.imshow(utils.normalize_image(image), cmap=plt.cm.Greys_r)
  ax2.set_title("FBP image")
  ax2.imshow(utils.normalize_image(reconstruction_fbp), cmap=plt.cm.Greys_r)
  ax3.set_title("Reconstruction error\nFiltered back projection")
  ax3.imshow(error, cmap=plt.cm.Greys_r, **imkwargs)
  plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: FBP rms reconstruction error: 0.105
[[file:./.ob-jupyter/aae84632d2a803a7aac0bf7c4eb669df0b4008ec.png]]
:END:

Here, we retrieve Mayo training data in pairs (full dose, quarter dose), and compare a few samples.
#+ATTR_LATEX: :options frame=single, linenos, breaklines, tabsize=2
#+begin_src jupyter-python :session py :exports both :async yes :eval never-export
  import mayo
  import numpy as np
  import matplotlib.pyplot as plt

  import utils

  path='/Users/huiyuanchua/Documents/data/Mayo_Grand_Challenge/Patient_Data/Training_Image_Data/3mm B30'
  training_data = mayo.get_training_data(path, 112, 113) # pull the middle slices of each patient
  fd_ima, qd_ima = training_data[len(training_data) // 2]
  noise = fd_ima - qd_ima

  print(f'noise: {np.sqrt(np.mean(noise**2)):.3g}')

  imkwargs = dict(vmin=-0.2, vmax=0.2)
  fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(8, 4.5), sharex=True, sharey=True)
  _ = fig.tight_layout()
  _ = ax1.set_title("full dose image")
  _ = ax1.imshow(utils.normalize_image(fd_ima), cmap=plt.cm.Greys_r)
  _ = ax2.set_title("quarter dose image")
  _ = ax2.imshow(utils.normalize_image(qd_ima), cmap=plt.cm.Greys_r)
  _ = ax3.set_title("noise")
  _ = ax3.imshow(noise, cmap=plt.cm.Greys_r, **imkwargs)
  plt.show()

#+end_src

#+RESULTS:
:RESULTS:
: loading patient data: 100% 10/10 [00:00<00:00, 65.98it/s]
:
: noise: 0.00468
[[file:./.ob-jupyter/87b4f042c9bbc291a64110740ae09645af97d617.png]]
:END:

Next, we explore if we can reconstruct images from Mayo's full dose projections (Helical scans), and how it compares with the provided images reconstructed using commercial software (weighted FBP).
#+ATTR_LATEX: :options frame=single, linenos, breaklines, tabsize=2
#+begin_src jupyter-python :session py :exports both :async yes :eval never-export
  #from pydicom import dcmread
  #import matplotlib.pyplot as plt
  #import numpy as np
  #from pathlib import Path

  path='/home/huiyuanchua/Documents/data/Mayo_Grand_Challenge/Patient_Data'

  dcm_path=f'{path}/Training_Projection_Data/L067'
  dcm_fd_path=f'{dcm_path}/DICOM-CT-PD_FD'

  import odl
  from odl.contrib.datasets.ct import mayo

  mayo_dir = '/home/huiyuanchua/Documents/data/Mayo_Grand_Challenge/Patient_Data'  # replace with your local folder

  # Load reference reconstruction
  #volume_folder = mayo_dir + '/Training Cases/L067/full_1mm_sharp'
  volume_folder = mayo_dir + '/Training_Image_Data/3mm B30/full_3mm/L067/full_3mm'
  partition, volume = mayo.load_reconstruction(volume_folder)

  # Load a subset of the projection data
  #data_folder = mayo_dir + '/Training Cases/L067/full_DICOM-CT-PD'
  data_folder = mayo_dir + '/Training_Projection_Data/L067/DICOM-CT-PD_FD'
  geometry, proj_data = mayo.load_projections(data_folder,
                                            indices=slice(20000, 28000))

  # Reconstruction space and ray transform
  space = odl.uniform_discr_frompartition(partition, dtype='float32')
  ray_trafo = odl.tomo.RayTransform(space, geometry)

  # Define FBP operator
  fbp = odl.tomo.fbp_op(ray_trafo, padding=True)

  # Tam-Danielsson window to handle redundant data
  td_window = odl.tomo.tam_danielson_window(ray_trafo, n_pi=3)

  # Calculate FBP reconstruction
  fbp_result = fbp(td_window * proj_data)

  # Compare the computed recon to reference reconstruction (coronal slice)
  ref = space.element(volume)
  fbp_result.show('Recon (coronal)', clim=[0.7, 1.3])
  ref.show('Reference (coronal)', clim=[0.7, 1.3])
  (ref - fbp_result).show('Diff (coronal)', clim=[-0.1, 0.1])

  # Also visualize sagittal slice (note that we only used a subset)
  coords = [0, None, None]
  fbp_result.show('Recon (sagittal)', clim=[0.7, 1.3], coords=coords)
  ref.show('Reference (sagittal)', clim=[0.7, 1.3], coords=coords)
  (ref - fbp_result).show('Diff (sagittal)', clim=[-0.1, 0.1], coords=coords)

  #n = 3
  #sinograms=[]
  #sinogram_batch = np.random.choice(dcm_files, size=n**2, replace=False)
  #fig, axs = plt.subplots(n, n, figsize=(3 *n, 3 * n), sharex=True, sharey=True)
  #_ = fig.tight_layout()
  #for i, sinogram_file in enumerate(sinogram_batch):
  #    dcm = dcmread(sinogram_file)
  #    sinogram = dcm.pixel_array
  #    sinograms.append(sinogram)
  #    ax = axs[i // n][i % n]
  #    ax.imshow(sinogram, cmap=plt.cm.Greys_r,)
  #plt.show()
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example
  [0;31m[0m
  [0;31mImportError[0mTraceback (most recent call last)
  [0;32m~/miniconda3/envs/odlenv/lib/python3.6/site-packages/dask/array/chunk.py[0m in [0;36m<module>[0;34m[0m
  [1;32m     14[0m [0;32mtry[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
  [0;32m---> 15[0;31m     [0;32mfrom[0m [0mnumpy[0m [0;32mimport[0m [0mtake_along_axis[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     16[0m [0;32mexcept[0m [0mImportError[0m[0;34m:[0m  [0;31m# pragma: no cover[0m[0;34m[0m[0;34m[0m[0m

  [0;31mImportError[0m: cannot import name 'take_along_axis'

  During handling of the above exception, another exception occurred:

  [0;31mAttributeError[0mTraceback (most recent call last)
  [0;32m<ipython-input-1-c46f9b485a61>[0m in [0;36m<module>[0;34m[0m
  [1;32m      9[0m [0mdcm_fd_path[0m[0;34m=[0m[0;34mf'{dcm_path}/DICOM-CT-PD_FD'[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     10[0m [0;34m[0m[0m
  [0;32m---> 11[0;31m [0;32mimport[0m [0modl[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     12[0m [0;32mfrom[0m [0modl[0m[0;34m.[0m[0mcontrib[0m[0;34m.[0m[0mdatasets[0m[0;34m.[0m[0mct[0m [0;32mimport[0m [0mmayo[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     13[0m [0;34m[0m[0m

  [0;32m~/miniconda3/envs/odlenv/lib/python3.6/site-packages/odl/__init__.py[0m in [0;36m<module>[0;34m[0m
  [1;32m     64[0m [0;32mfrom[0m [0;34m.[0m [0;32mimport[0m [0mphantom[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     65[0m [0;32mfrom[0m [0;34m.[0m [0;32mimport[0m [0msolvers[0m[0;34m[0m[0;34m[0m[0m
  [0;32m---> 66[0;31m [0;32mfrom[0m [0;34m.[0m [0;32mimport[0m [0mtomo[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     67[0m [0;32mfrom[0m [0;34m.[0m [0;32mimport[0m [0mtrafos[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     68[0m [0;32mfrom[0m [0;34m.[0m [0;32mimport[0m [0mufunc_ops[0m[0;34m[0m[0;34m[0m[0m

  [0;32m~/miniconda3/envs/odlenv/lib/python3.6/site-packages/odl/tomo/__init__.py[0m in [0;36m<module>[0;34m[0m
  [1;32m     17[0m [0m__all__[0m [0;34m+=[0m [0mgeometry[0m[0;34m.[0m[0m__all__[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     18[0m [0;34m[0m[0m
  [0;32m---> 19[0;31m [0;32mfrom[0m [0;34m.[0m[0mbackends[0m [0;32mimport[0m [0;34m*[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     20[0m [0m__all__[0m [0;34m+=[0m [0mbackends[0m[0;34m.[0m[0m__all__[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     21[0m [0;34m[0m[0m

  [0;32m~/miniconda3/envs/odlenv/lib/python3.6/site-packages/odl/tomo/backends/__init__.py[0m in [0;36m<module>[0;34m[0m
  [1;32m     22[0m [0m__all__[0m [0;34m+=[0m [0mastra_cuda[0m[0;34m.[0m[0m__all__[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     23[0m [0;34m[0m[0m
  [0;32m---> 24[0;31m [0;32mfrom[0m [0;34m.[0m[0mskimage_radon[0m [0;32mimport[0m [0;34m*[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     25[0m [0m__all__[0m [0;34m+=[0m [0mskimage_radon[0m[0;34m.[0m[0m__all__[0m[0;34m[0m[0;34m[0m[0m

  [0;32m~/miniconda3/envs/odlenv/lib/python3.6/site-packages/odl/tomo/backends/skimage_radon.py[0m in [0;36m<module>[0;34m[0m
  [1;32m     12[0m [0;32mimport[0m [0mnumpy[0m [0;32mas[0m [0mnp[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     13[0m [0;32mtry[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
  [0;32m---> 14[0;31m     [0;32mimport[0m [0mskimage[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     15[0m     [0mSKIMAGE_AVAILABLE[0m [0;34m=[0m [0;32mTrue[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     16[0m [0;32mexcept[0m [0mImportError[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

  [0;32m~/miniconda3/envs/odlenv/lib/python3.6/site-packages/skimage/__init__.py[0m in [0;36m<module>[0;34m[0m
  [1;32m    125[0m [0;34m[0m[0m
  [1;32m    126[0m     [0;31m# All skimage root imports go here[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  [0;32m--> 127[0;31m     from .util.dtype import (img_as_float32,
  [0m[1;32m    128[0m                              [0mimg_as_float64[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
  [1;32m    129[0m                              [0mimg_as_float[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m

  [0;32m~/miniconda3/envs/odlenv/lib/python3.6/site-packages/skimage/util/__init__.py[0m in [0;36m<module>[0;34m[0m
  [1;32m      4[0m [0;32mfrom[0m [0;34m.[0m[0mshape[0m [0;32mimport[0m [0mview_as_blocks[0m[0;34m,[0m [0mview_as_windows[0m[0;34m[0m[0;34m[0m[0m
  [1;32m      5[0m [0;32mfrom[0m [0;34m.[0m[0mnoise[0m [0;32mimport[0m [0mrandom_noise[0m[0;34m[0m[0;34m[0m[0m
  [0;32m----> 6[0;31m [0;32mfrom[0m [0;34m.[0m[0mapply_parallel[0m [0;32mimport[0m [0mapply_parallel[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m      7[0m [0;34m[0m[0m
  [1;32m      8[0m [0;32mfrom[0m [0;34m.[0m[0marraycrop[0m [0;32mimport[0m [0mcrop[0m[0;34m[0m[0;34m[0m[0m

  [0;32m~/miniconda3/envs/odlenv/lib/python3.6/site-packages/skimage/util/apply_parallel.py[0m in [0;36m<module>[0;34m[0m
  [1;32m      6[0m [0;34m[0m[0m
  [1;32m      7[0m [0;32mtry[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
  [0;32m----> 8[0;31m     [0;32mimport[0m [0mdask[0m[0;34m.[0m[0marray[0m [0;32mas[0m [0mda[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m      9[0m     [0mdask_available[0m [0;34m=[0m [0;32mTrue[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     10[0m [0;32mexcept[0m [0mImportError[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

  [0;32m~/miniconda3/envs/odlenv/lib/python3.6/site-packages/dask/array/__init__.py[0m in [0;36m<module>[0;34m[0m
  [1;32m      1[0m [0;32mtry[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
  [0;32m----> 2[0;31m     [0;32mfrom[0m [0;34m.[0m[0mblockwise[0m [0;32mimport[0m [0mblockwise[0m[0;34m,[0m [0matop[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m      3[0m     from .core import (
  [1;32m      4[0m         [0mArray[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
  [1;32m      5[0m         [0mblock[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m

  [0;32m~/miniconda3/envs/odlenv/lib/python3.6/site-packages/dask/array/blockwise.py[0m in [0;36m<module>[0;34m[0m
  [1;32m    286[0m [0;34m[0m[0m
  [1;32m    287[0m [0;34m[0m[0m
  [0;32m--> 288[0;31m [0;32mfrom[0m [0;34m.[0m[0mcore[0m [0;32mimport[0m [0mnew_da_object[0m[0;34m[0m[0;34m[0m[0m
  [0m
  [0;32m~/miniconda3/envs/odlenv/lib/python3.6/site-packages/dask/array/core.py[0m in [0;36m<module>[0;34m[0m
  [1;32m     20[0m [0;32mimport[0m [0mnumpy[0m [0;32mas[0m [0mnp[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     21[0m [0;34m[0m[0m
  [0;32m---> 22[0;31m [0;32mfrom[0m [0;34m.[0m [0;32mimport[0m [0mchunk[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     23[0m [0;32mfrom[0m [0;34m.[0m[0;34m.[0m [0;32mimport[0m [0mconfig[0m[0;34m,[0m [0mcompute[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     24[0m from ..base import (

  [0;32m~/miniconda3/envs/odlenv/lib/python3.6/site-packages/dask/array/chunk.py[0m in [0;36m<module>[0;34m[0m
  [1;32m     15[0m     [0;32mfrom[0m [0mnumpy[0m [0;32mimport[0m [0mtake_along_axis[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     16[0m [0;32mexcept[0m [0mImportError[0m[0;34m:[0m  [0;31m# pragma: no cover[0m[0;34m[0m[0;34m[0m[0m
  [0;32m---> 17[0;31m     [0mtake_along_axis[0m [0;34m=[0m [0mnpcompat[0m[0;34m.[0m[0mtake_along_axis[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     18[0m [0;34m[0m[0m
  [1;32m     19[0m [0;34m[0m[0m

  [0;31mAttributeError[0m: module 'dask.array.numpy_compat' has no attribute 'take_along_axis'
#+end_example
:END:

* DDPM
DDPM specific parameters.
#+ATTR_LATEX: :options frame=single, linenos, breaklines, tabsize=2
#+begin_src jupyter-python :session py :exports both :async yes :eval never-export
  import os

  SEED=42
  MIN_BETA, MAX_BETA = 1e-4, 0.02
  K = 200
  N_EPOCH = 30
  BATCH_SIZE = 10
  PROJECT_DIR=os.path.abspath('.')
#+end_src

#+RESULTS:

** Training
First, let's make sure we have the correct shapes for training data inputs and predictions.
#+ATTR_LATEX: :options frame=single, linenos, breaklines, tabsize=2
#+begin_src jupyter-python :session py :exports both :async yes :eval never-export
    from jax import random
    import jax.numpy as jnp
    import numpy as np

    import utils
    import mayo

    key = random.PRNGKey(42)
    state = utils.create_training_state(key=key)
    path='/Users/huiyuanchua/Documents/data/Mayo_Grand_Challenge/Patient_Data/Training_Image_Data/3mm B30'
    training_data = mayo.get_training_data(path, 112, 113)

    x_0_fd = training_data[:, 0]
    x_0_ld = training_data[:, 1]
    x_k = jnp.concatenate((x_0_fd, x_0_ld), axis=-1)
    n = training_data.shape[0]
    k = random.choice(key, np.arange(200), shape=(n,))
    predictions = state.apply_fn(state.params, x_k, k)
    print(f'training input shape (x_t, t): {x_k.shape}, {k.shape}')
    print(f'predictions shape (x_0): {predictions.shape}')
#+end_src

#+RESULTS:
:RESULTS:
: loading patient data: 100% 10/10 [00:00<00:00, 76.08it/s]
:
: training input shape (x_t, t): (10, 128, 128, 2) {(10,)}
: predictions shape (x_0): (10, 128, 128, 1)
:END:

Training is performed via a python script [[file:train_ddpm.py]]. We examine the average epoch loss with the following code:
#+ATTR_LATEX: :options frame=single, linenos, breaklines, tabsize=2
#+begin_src jupyter-python :session py :exports both :async yes :eval never-export
  import pandas as pd
  import matplotlib.pyplot as plt
  import seaborn as sns
  import os

  import utils

  PROJECT_DIR=os.path.abspath('.')
  ddpm_loss_log = utils.load_loss_log(f'{PROJECT_DIR}/ddpm_loss_log.npy')

  # plot losses
  df = pd.DataFrame([(int(x), float(y)) for x, _, y in ddpm_loss_log], columns=['epoch', 'loss'])
  sns.relplot(df, x='epoch', y='loss', kind='line')

  _ = plt.tight_layout()
  _ = plt.show()
#+end_src

#+RESULTS:
:RESULTS:
#+attr_org: :width 484
[[file:./.ob-jupyter/98b946f3ee6dbe5becc443cc009687eff61363d4.png]]
:END:

** Sampling
We sample images from the trained DDPM model.
#+ATTR_LATEX: :options frame=single, linenos, breaklines, tabsize=2
#+begin_src jupyter-python :session py :exports both :async yes :eval never-export
  import matplotlib.pyplot as plt
  import optax
  from jax import random
  import jax.numpy as jnp
  from tqdm import tqdm
  import numpy as np

  import utils
  import mayo

  def sample(state, condition, n, betas, key):
    # random white noise X_T
    key, subkey = random.split(key)
    x_k = random.normal(subkey, shape=(n, condition.shape[0], condition.shape[1], 1))

    #dts = np.array([ts[i] - ts[i-1] for i in range(1, steps+1)])
    #betas = 1- np.exp(-dts)
    alphas = 1 - betas
    alpha_bars = jnp.cumprod(alphas)
    #alpha_bars = jnp.array([alphas[:i+1].prod() for i in range(len(alphas))]) # workaround for metal problem with jnp.cumprod

    condition = np.repeat(condition.reshape((1, condition.shape[0], condition.shape[1], 1)), n, axis=0)

    # sample in reverse from T=10 to 0.0 in evenly distributed steps
    #for i in tqdm(range(steps)[::-1]):
    for k in range(len(betas))[::-1]:
      alpha = alphas[k]
      beta = betas[k]
      alpha_bar_k = alpha_bars[k]

      key, subkey = random.split(key)
      z = jnp.where(k > 1, random.normal(subkey, shape=x_k.shape), jnp.zeros_like(x_k))
      sigma_k = jnp.sqrt(beta) # option 1; see DDPM 3.2
      #sigma_k = jnp.sqrt((1-alpha_bars[k-1])/(1 - alpha_bar_k) * beta) # option 2; see DDPM 3.2

      inputs = jnp.concatenate((x_k, condition), axis=-1)

      x_k = 1/jnp.sqrt(alpha) * (x_k - beta/jnp.sqrt(1 - alpha_bar_k) * state.apply_fn(state.params, inputs, k * jnp.ones((x_k.shape[0], )))) + sigma_k * z

      #x_k = jnp.clip(x_k, -1., 1.) # should we clip ...
      x_t = jnp.clip(x_t, -3., 3.)
      #x_t = normalize_to_neg_one_to_one(x_t) # or scale?

    return x_k

  key = random.PRNGKey(SEED)

  # use the best params
  file_path, epoch, step, loss = utils.find_latest_pytree(f'{PROJECT_DIR}/ddpm_params_*.npy')
  ddpm_state = utils.create_training_state(params_file=f'{PROJECT_DIR}/ddpm_params_{epoch}_{step}_{loss}.npy')
  print(f'using parameters from epoch {epoch} with loss {loss}')

  betas = jnp.linspace(MIN_BETA, MAX_BETA, K)

  path='/Users/huiyuanchua/Documents/data/Mayo_Grand_Challenge/Patient_Data/Training_Image_Data/3mm B30'
  training_data = mayo.get_training_data(path, 112, 113)
  n = (len(training_data) // 10) * 9
  test_data = training_data[n:]
  test_data = []
  n_samples = 1000

  fd_data, ld_data = training_data[0]
  result = np.zeros(fd_data.shape)

  for i in tqdm(range(n_samples // BATCH_SIZE)):
    # generate x_0 from noise
    key, subkey = random.split(key)
    x_0_tilde = sample(ddpm_state, ld_data, BATCH_SIZE, betas, subkey)

    result = result + np.sum(x_0_tilde, axis=0)

  result = result / n_samples
  error = fd_data - result
  print(f'error: {np.mean(error)}')

  # plot and compare last sample
  imkwargs = dict(vmin=-0.2, vmax=0.2)
  fig, axs = plt.subplots(2, 2, figsize=(8, 4.5), sharex=True, sharey=True)
  _ = fig.tight_layout()
  _ = axs[0, 0].set_title("full dose image")
  _ = axs[0, 0].imshow(utils.normalize_image_to_greyscale(fd_data), cmap=plt.cm.Greys_r)
  _ = axs[0, 1].set_title("quarter dose image")
  _ = axs[0, 1].imshow(utils.normalize_image_to_greyscale(ld_data), cmap=plt.cm.Greys_r)
  _ = axs[1, 0].set_title("DDPM")
  _ = axs[1, 0].imshow(utils.normalize_image_to_greyscale(result), cmap=plt.cm.Greys_r)
  _ = axs[1, 1].set_title("error")
  _ = axs[1, 1].imshow(error, cmap=plt.cm.Greys_r, **imkwargs)

  plt.show()
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
  Platform 'METAL' is experimental and not all JAX functionality may be correctly supported!
  2024-03-23 12:48:02.264962: W pjrt_plugin/src/mps_client.cc:563] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported!
  Metal device set to: Apple M1
  using parameters from epoch 22 with loss 0.00825
  loading patient data: 100% 10/10 [00:00<00:00, 59.47it/s]

  100% 200/200 [03:55<00:00,  1.18s/it]

  100% 200/200 [03:50<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:50<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:54<00:00,  1.17s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:51<00:00,  1.16s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:52<00:00,  1.16s/it]
100% 200/200 [03:50<00:00,  1.15s/it]
100% 200/200 [03:52<00:00,  1.16s/it]
100% 200/200 [03:51<00:00,  1.16s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:51<00:00,  1.16s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:48<00:00,  1.14s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:52<00:00,  1.16s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:52<00:00,  1.16s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:50<00:00,  1.15s/it]
100% 200/200 [03:53<00:00,  1.17s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:52<00:00,  1.16s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:52<00:00,  1.16s/it]
100% 200/200 [03:50<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:50<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:50<00:00,  1.15s/it]
100% 200/200 [03:52<00:00,  1.16s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:49<00:00,  1.15s/it]
100% 200/200 [03:50<00:00,  1.15s/it]
100% 200/200 [03:52<00:00,  1.16s/it]
100% 200/200 [03:50<00:00,  1.15s/it]
100% 200/200 [03:50<00:00,  1.15s/it]
100% 200/200 [03:50<00:00,  1.15s/it]
100% 200/200 [03:50<00:00,  1.15s/it]
100% 200/200 [03:50<00:00,  1.15s/it]
  error: 0.11948700249195099
#+end_example
[[file:./.ob-jupyter/da8b230284e5503497e7e30a76f4e73bb3cc5842.png]]
:END:

* CEM
#+ATTR_LATEX: :options frame=single, linenos, breaklines, tabsize=2
#+begin_src jupyter-python :session py :exports both :async yes :eval never-export
  import os

  SEED=42
  T=10.
  K=1000
  BATCH_SIZE = 10
  PROJECT_DIR=os.path.abspath('.')
#+end_src

#+RESULTS:
** Training
Training is performed via a python script [[file:train_cem.py]]. We examine the average epoch loss with the following code:
#+ATTR_LATEX: :options frame=single, linenos, breaklines, tabsize=2
#+begin_src jupyter-python :session py :exports both :async yes :eval never-export
  import pandas as pd
  import matplotlib.pyplot as plt
  import seaborn as sns

  import utils

  cem_loss_log = utils.load_loss_log(f'{PROJECT_DIR}/cem_loss_log.npy')

  # plot losses
  df = pd.DataFrame([(int(x), float(y)) for x, _, y in cem_loss_log], columns=['epoch', 'loss'])
  sns.relplot(df, x='epoch', y='loss', kind='line')

  _ = plt.tight_layout()
  _ = plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: /Users/huiyuanchua/miniconda3/envs/mlenv-metal/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
:   with pd.option_context('mode.use_inf_as_na', True):
: /Users/huiyuanchua/miniconda3/envs/mlenv-metal/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
:   with pd.option_context('mode.use_inf_as_na', True):
#+attr_org: :width 484
[[file:./.ob-jupyter/9116398acd8b24680bfcc5c9c8c5b48242340955.png]]
:END:

** Sampling
We sample images from the trained CEM model.
#+ATTR_LATEX: :options frame=single, linenos, breaklines, tabsize=2
#+begin_src jupyter-python :session py :exports both :async yes :eval never-export
  import matplotlib.pyplot as plt
  import optax
  from jax import random
  import jax.numpy as jnp
  from tqdm import tqdm
  import numpy as np

  import utils
  from unet import Unet
  import mayo

  def sample(state, condition, n, ts, key):
    # random white noise X_T
    key, subkey = random.split(key)
    x_t = random.normal(subkey, shape=(n, condition.shape[0], condition.shape[1], 1))

    condition = np.repeat(condition.reshape((1, condition.shape[0], condition.shape[1], 1)), n, axis=0)

    step=0

    for k in range(len(ts))[::-1]:
      key, subkey = random.split(key)
      z = random.normal(subkey, shape=x_t.shape)

      t = ts[k]
      dt = jnp.where(k > 0, t - ts[k-1], 0.)

      inputs = jnp.concatenate((x_t, condition), axis=-1)

      f_theta = state.apply_fn(state.params, inputs, t * jnp.ones((n,)))

      # equation (40)
      s_theta = jnp.where(k > 0, x_t/(1-jnp.exp(-t))  - jnp.exp(-t/2)/(1-jnp.exp(-t)) * f_theta,  0.)

      # equation (24)
      x_t_bar = x_t - dt * s_theta
      x_t = jnp.exp(dt/2) * x_t_bar + jnp.sqrt(1-jnp.exp(-dt)) * z

      x_t = jnp.clip(x_t, -1., 1.) # should we clip ...
      #x_t = normalize_to_neg_one_to_one(x_t) # or scale?

      step=step+1

    return x_t

  key = random.PRNGKey(SEED)
  n_samples = 100
  samples = []

  # use the best params
  file_path, epoch, step, loss = utils.find_latest_pytree(f'{PROJECT_DIR}/cem_params_*.npy')
  cem_state = utils.create_training_state(params_file=file_path)
  print(f'using parameters from epoch {epoch} with loss {loss}')

  ts = utils.exponential_time_schedule(T, K)
  #path='/home/gpu_user1/Documents/data/Mayo_Grand_Challenge/Patient_Data/Training_Image_Data/3mm B30'
  path='/Users/huiyuanchua/Documents/data/Mayo_Grand_Challenge/Patient_Data/Training_Image_Data/3mm B30'
  training_data = mayo.get_training_data(path, 36, 37)
  n = (len(training_data) // 10) * 9
  test_data = training_data[n:]

  fd_data, ld_data = training_data[0]
  result = np.zeros(fd_data.shape)

  # define window parameters for abdominal scans
  # range from -125HU to 225HU centered around 50HU
  minval = -125
  maxval = 225
  midval = (maxval - minval)/2 + minval # 50 HU
  wd = lambda x : utils.window_image(x, minval, maxval)

  # rescale [-125HU, 225HU] to [-1, 1]
  ld_data_windowed = (np.clip(ld_data, minval, maxval) - midval)/((maxval - minval)/2)

  for i in tqdm(range(n_samples // BATCH_SIZE)):
    # generate x_0 from noise
    key, subkey = random.split(key)
    x_0_tilde = sample(cem_state, ld_data_windowed, BATCH_SIZE, ts, subkey)

    # rescale in HU
    x_0_tilde = x_0_tilde * (maxval - minval)/2 + midval

    samples.append(x_0_tilde)

  sample_mean = np.mean(samples, axis=0)
  sample_variance = np.var(samples, axis=0)

  # plot the data
  imkwargs = dict(vmin=-0.2, vmax=0.2)
  fig, axs = plt.subplots(2, 2, figsize=(8, 4.5), sharex=True, sharey=True)
  _ = fig.tight_layout()
  _ = axs[0, 0].set_title("full dose image")
  _ = axs[0, 0].imshow(wd(fd_data), cmap=plt.cm.Greys_r)
  _ = axs[0, 1].set_title("quarter dose image")
  _ = axs[0, 1].imshow(wd(ld_data), cmap=plt.cm.Greys_r)
  _ = axs[1, 0].set_title("CEM mean")
  _ = axs[1, 0].imshow(wd(sample_mean), cmap=plt.cm.Greys_r)
  _ = axs[1, 1].set_title("CEM variance")
  _ = axs[1, 1].imshow(wd(sample_variance), **imkwargs)

  plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: Platform 'METAL' is experimental and not all JAX functionality may be correctly supported!
: 2024-03-28 18:25:02.399715: W pjrt_plugin/src/mps_client.cc:563] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported!
: Metal device set to: Apple M1
: using parameters from epoch 59 with loss 0.00021
: loading patient data: 100% 10/10 [00:00<00:00, 61.59it/s]
:
: 100% 10/10 [3:13:29<00:00, 1161.00s/it]
:
# [goto error]
#+begin_example
  [0;31m---------------------------------------------------------------------------[0m
  [0;31mTypeError[0m                                 Traceback (most recent call last)
  Cell [0;32mIn[3], line 97[0m
  [1;32m     95[0m _ [38;5;241m=[39m axs[[38;5;241m0[39m, [38;5;241m1[39m][38;5;241m.[39mimshow(wd(ld_data), cmap[38;5;241m=[39mplt[38;5;241m.[39mcm[38;5;241m.[39mGreys_r)
  [1;32m     96[0m _ [38;5;241m=[39m axs[[38;5;241m1[39m, [38;5;241m0[39m][38;5;241m.[39mset_title([38;5;124m"[39m[38;5;124mCEM mean[39m[38;5;124m"[39m)
  [0;32m---> 97[0m _ [38;5;241m=[39m axs[[38;5;241m1[39m, [38;5;241m0[39m][38;5;241m.[39mimshow(wd(sample_mean), cmap[38;5;241m=[39mplt[38;5;241m.[39mcm[38;5;241m.[39mGreys_r)
  [1;32m     98[0m _ [38;5;241m=[39m axs[[38;5;241m1[39m, [38;5;241m1[39m][38;5;241m.[39mset_title([38;5;124m"[39m[38;5;124mCEM variance[39m[38;5;124m"[39m)
  [1;32m     99[0m _ [38;5;241m=[39m axs[[38;5;241m1[39m, [38;5;241m1[39m][38;5;241m.[39mimshow(wd(sample_variance), [38;5;241m*[39m[38;5;241m*[39mimkwargs)

  File [0;32m~/miniconda3/envs/mlenv-metal/lib/python3.11/site-packages/matplotlib/__init__.py:1465[0m, in [0;36m_preprocess_data.<locals>.inner[0;34m(ax, data, *args, **kwargs)[0m
  [1;32m   1462[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(func)
  [1;32m   1463[0m [38;5;28;01mdef[39;00m [38;5;21minner[39m(ax, [38;5;241m*[39margs, data[38;5;241m=[39m[38;5;28;01mNone[39;00m, [38;5;241m*[39m[38;5;241m*[39mkwargs):
  [1;32m   1464[0m     [38;5;28;01mif[39;00m data [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
  [0;32m-> 1465[0m         [38;5;28;01mreturn[39;00m func(ax, [38;5;241m*[39m[38;5;28mmap[39m(sanitize_sequence, args), [38;5;241m*[39m[38;5;241m*[39mkwargs)
  [1;32m   1467[0m     bound [38;5;241m=[39m new_sig[38;5;241m.[39mbind(ax, [38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)
  [1;32m   1468[0m     auto_label [38;5;241m=[39m (bound[38;5;241m.[39marguments[38;5;241m.[39mget(label_namer)
  [1;32m   1469[0m                   [38;5;129;01mor[39;00m bound[38;5;241m.[39mkwargs[38;5;241m.[39mget(label_namer))

  File [0;32m~/miniconda3/envs/mlenv-metal/lib/python3.11/site-packages/matplotlib/axes/_axes.py:5751[0m, in [0;36mAxes.imshow[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)[0m
  [1;32m   5748[0m [38;5;28;01mif[39;00m aspect [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m:
  [1;32m   5749[0m     [38;5;28mself[39m[38;5;241m.[39mset_aspect(aspect)
  [0;32m-> 5751[0m im[38;5;241m.[39mset_data(X)
  [1;32m   5752[0m im[38;5;241m.[39mset_alpha(alpha)
  [1;32m   5753[0m [38;5;28;01mif[39;00m im[38;5;241m.[39mget_clip_path() [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
  [1;32m   5754[0m     [38;5;66;03m# image does not already have clipping set, clip to axes patch[39;00m

  File [0;32m~/miniconda3/envs/mlenv-metal/lib/python3.11/site-packages/matplotlib/image.py:723[0m, in [0;36m_ImageBase.set_data[0;34m(self, A)[0m
  [1;32m    721[0m [38;5;28;01mif[39;00m [38;5;28misinstance[39m(A, PIL[38;5;241m.[39mImage[38;5;241m.[39mImage):
  [1;32m    722[0m     A [38;5;241m=[39m pil_to_array(A)  [38;5;66;03m# Needed e.g. to apply png palette.[39;00m
  [0;32m--> 723[0m [38;5;28mself[39m[38;5;241m.[39m_A [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39m_normalize_image_array(A)
  [1;32m    724[0m [38;5;28mself[39m[38;5;241m.[39m_imcache [38;5;241m=[39m [38;5;28;01mNone[39;00m
  [1;32m    725[0m [38;5;28mself[39m[38;5;241m.[39mstale [38;5;241m=[39m [38;5;28;01mTrue[39;00m

  File [0;32m~/miniconda3/envs/mlenv-metal/lib/python3.11/site-packages/matplotlib/image.py:693[0m, in [0;36m_ImageBase._normalize_image_array[0;34m(A)[0m
  [1;32m    691[0m     A [38;5;241m=[39m A[38;5;241m.[39msqueeze([38;5;241m-[39m[38;5;241m1[39m)  [38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.[39;00m
  [1;32m    692[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m (A[38;5;241m.[39mndim [38;5;241m==[39m [38;5;241m2[39m [38;5;129;01mor[39;00m A[38;5;241m.[39mndim [38;5;241m==[39m [38;5;241m3[39m [38;5;129;01mand[39;00m A[38;5;241m.[39mshape[[38;5;241m-[39m[38;5;241m1[39m] [38;5;129;01min[39;00m [[38;5;241m3[39m, [38;5;241m4[39m]):
  [0;32m--> 693[0m     [38;5;28;01mraise[39;00m [38;5;167;01mTypeError[39;00m([38;5;124mf[39m[38;5;124m"[39m[38;5;124mInvalid shape [39m[38;5;132;01m{[39;00mA[38;5;241m.[39mshape[38;5;132;01m}[39;00m[38;5;124m for image data[39m[38;5;124m"[39m)
  [1;32m    694[0m [38;5;28;01mif[39;00m A[38;5;241m.[39mndim [38;5;241m==[39m [38;5;241m3[39m:
  [1;32m    695[0m     [38;5;66;03m# If the input data has values outside the valid range (after[39;00m
  [1;32m    696[0m     [38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds[39;00m
  [1;32m    697[0m     [38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and[39;00m
  [1;32m    698[0m     [38;5;66;03m# making reliable interpretation impossible.[39;00m
  [1;32m    699[0m     high [38;5;241m=[39m [38;5;241m255[39m [38;5;28;01mif[39;00m np[38;5;241m.[39missubdtype(A[38;5;241m.[39mdtype, np[38;5;241m.[39minteger) [38;5;28;01melse[39;00m [38;5;241m1[39m

  [0;31mTypeError[0m: Invalid shape (10, 128, 128, 1) for image data
#+end_example
[[file:./.ob-jupyter/7374050d3fe3599adae3cd3169972512008ea652.png]]
:END:
:END:

* Reads
- Quick summary on CR reconstruciton and Helical CT http://xrayphysics.com/ctsim.html
- Example to perform simple image reconstruction using scikit-image https://scikit-image.org/docs/stable/auto_examples/transform/plot_radon_transform.html
- Python code to reconstruct image from helical scans? https://github.com/dzwiedzn7/filtered-back-projection/blob/master/tompy.py
- Python code to simulate thick slice images from Helical scans https://github.com/Feanor007/Thin2Thick
- Python library for Tomography https://pypi.org/project/algotom/
- C code for Model-Based Iterative Reconstruciton code for Multi-Slice Helical Geometry https://github.com/cabouman/OpenMBIR-Index/blob/master/README.md
- General summary for 3D image reconstruction https://humanhealth.iaea.org/HHW/MedicalPhysics/NuclearMedicine/ImageAnalysis/3Dimagereconstruction/index.html
- Pyro-NN: Generalized Python code for image reconstruction using deep learning implemented in Tensorflow code https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6899669/
- TomoPy: python library for tomographic data analysis https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4181643/
- Powerpoint presentation on CT image reconstruction http://www.sci.utah.edu/~shireen/pdfs/tutorials/Elhabian_CT09.pdf
- Astra toolbox: python matlab library for 2D/3D tomography https://github.com/astra-toolbox/astra-toolbox
- Operator Discretization Library (ODL): used by authors of the paper and has example Python code to reconstruct image from Helical scans https://github.com/odlgroup/odl/tree/master/examples/tomo
- matlab code for simple low-dose CT samples simulation https://github.com/smuzd/LD-CT-simulation/tree/master
- matlab code for 2nd winner from 2016 Mayo Grand Challenge https://github.com/jongcye/deeplearningLDCT/tree/master
- Jupyter code to add white noise to CT scans. May have code to reconstruct images from Helical scans? https://github.com/ayaanzhaque/Noise2Quality
- Alternative CT training data? https://www.kaggle.com/c/data-science-bowl-2017/overview
- Alternative CT training data from piglets? https://link.springer.com/article/10.1007/s10278-018-0056-0
* References
#+PRINT_BIBLIOGRAPHY:
