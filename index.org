#+TITLE: Examination of DDPM and CEM on Bayesian Inversion problems.
#+setupfile: ~/.emacs.d/setupfile.org

This is an investigation into the use of diffusion models to solve the problem discussed in [cite:@adler2018_deep_bayesian_inversion]. Specifically, we use DDPM and CEM to model the problems.

* Data
We use data from 2016 Low Dose CT Grand Challenge (https://www.aapm.org/grandchallenge/lowdosect/#testDatasets). Training data is downloaded from box at https://aapm.app.box.com/s/eaw4jddb53keg1bptavvvd1sf4x3pe9h.
#+ATTR_LATEX: :options frame=single, linenos, breaklines, tabsize=2
#+begin_src jupyter-python :session py :exports both :async yes :eval never-export
  import mnist

  normalized_images = mnist.get_training_data()
#+end_src

#+RESULTS:
: 2024-03-06 20:33:52.902441: W external/xla/xla/service/gpu/nvptx_compiler.cc:742] The NVIDIA driver's CUDA version is 12.0 which is older than the ptxas CUDA version (12.4.99). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.

* DDPM
DDPM specific parameters.
#+ATTR_LATEX: :options frame=single, linenos, breaklines, tabsize=2
#+begin_src jupyter-python :session py :exports both :async yes :eval never-export
  import os

  SEED=42
  MIN_BETA, MAX_BETA = 1e-4, 0.02
  K = 1000
  N_EPOCH = 30
  BATCH_SIZE = 10
  PROJECT_DIR=os.path.abspath('.')
#+end_src

#+RESULTS:

** Training
Training is performed via a python script [[file:train_ddpm.py]]. We examine the average epoch loss with the following code:
#+ATTR_LATEX: :options frame=single, linenos, breaklines, tabsize=2
#+begin_src jupyter-python :session py :exports both :async yes :eval never-export
  import pandas as pd
  import matplotlib.pyplot as plt
  import seaborn as sns

  import utils

  ddpm_loss_log = utils.load_loss_log(f'{PROJECT_DIR}/ddpm_loss_log.npy')

  # plot losses
  df = pd.DataFrame([(int(x), float(y)) for x, _, y in ddpm_loss_log], columns=['epoch', 'loss'])
  sns.relplot(df, x='epoch', y='loss', kind='line')

  _ = plt.tight_layout()
  _ = plt.show()
#+end_src

#+RESULTS:
:RESULTS:
#+attr_org: :width 484
[[file:./.ob-jupyter/5e1419e04ffa8611806495b89d9dcbfafe0c4049.png]]
:END:

** Sampling
We sample images from the trained DDPM model.
#+ATTR_LATEX: :options frame=single, linenos, breaklines, tabsize=2
#+begin_src jupyter-python :session py :exports both :async yes :eval never-export
  import matplotlib.pyplot as plt
  import optax
  from jax import random
  import jax.numpy as jnp
  from tqdm import tqdm

  import utils
  from unet import Unet

  def sample(state, n, betas, key):
    # random white noise X_T
    key, subkey = random.split(key)
    x_k = random.normal(subkey, shape=(n, 28, 28, 1))

    #dts = np.array([ts[i] - ts[i-1] for i in range(1, steps+1)])
    #betas = 1- np.exp(-dts)
    alphas = 1 - betas
    alpha_bars = jnp.cumprod(alphas)
    #alpha_bars = jnp.array([alphas[:i+1].prod() for i in range(len(alphas))]) # workaround for metal problem with jnp.cumprod

    # sample in reverse from T=10 to 0.0 in evenly distributed steps
    #for i in tqdm(range(steps)[::-1]):
    for k in tqdm(range(len(betas))[::-1]):
      alpha = alphas[k]
      beta = betas[k]
      alpha_bar_k = alpha_bars[k]

      key, subkey = random.split(key)
      z = jnp.where(k > 1, random.normal(subkey, shape=x_k.shape), jnp.zeros_like(x_k))
      sigma_k = jnp.sqrt(beta) # option 1; see DDPM 3.2
      #sigma_k = jnp.sqrt((1-alpha_bars[k-1])/(1 - alpha_bar_k) * beta) # option 2; see DDPM 3.2

      x_k = 1/jnp.sqrt(alpha) * (x_k - beta/jnp.sqrt(1 - alpha_bar_k) * state.apply_fn(state.params, x_k, k * jnp.ones((x_k.shape[0], )))) + sigma_k * z

      x_k = jnp.clip(x_k, -1., 1.) # should we clip ...
      #x_t = normalize_to_neg_one_to_one(x_t) # or scale?

    return x_k

  key = random.PRNGKey(SEED)

  # use the best params
  file_path, epoch, step, loss = utils.find_latest_pytree(f'{PROJECT_DIR}/ddpm_params_*.npy')
  ddpm_state = utils.create_training_state(params_file=f'{PROJECT_DIR}/ddpm_params_{epoch}_{step}_{loss}.npy')
  print(f'using parameters from epoch {epoch} with loss {loss}')

  betas = jnp.linspace(MIN_BETA, MAX_BETA, K)

  # generate x_0 from noise
  key, subkey = random.split(key)
  x_0_tilde = sample(ddpm_state, 4, betas, subkey)

  # plot the data
  utils.show_img_grid(utils.unnormalize_image(x_0_tilde))
#+end_src

#+RESULTS:
:RESULTS:
: 2024-03-07 08:56:45.170093: W external/xla/xla/service/gpu/nvptx_compiler.cc:742] The NVIDIA driver's CUDA version is 12.0 which is older than the ptxas CUDA version (12.4.99). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.
: using parameters from epoch 8 with loss 0.02443
: 100% 1000/1000 [14:06<00:00,  1.18it/s]
#+attr_org: :width 529
[[file:./.ob-jupyter/366e5ecd5190dcfa0dfd0afc39a52d418f35ab5e.png]]
:END:

* CEM
#+ATTR_LATEX: :options frame=single, linenos, breaklines, tabsize=2
#+begin_src jupyter-python :session py :exports both :async yes :eval never-export
  import os

  SEED=42
  T=10.
  K=1000
  BATCH_SIZE = 1000
  PROJECT_DIR=os.path.abspath('.')
#+end_src

#+RESULTS:
** Training
Training is performed via a python script [[file:train_cem.py]]. We examine the average epoch loss with the following code:
#+ATTR_LATEX: :options frame=single, linenos, breaklines, tabsize=2
#+begin_src jupyter-python :session py :exports both :async yes :eval never-export
  import pandas as pd
  import matplotlib.pyplot as plt
  import seaborn as sns

  import utils

  cem_loss_log = utils.load_loss_log(f'{PROJECT_DIR}/cem_loss_log.npy')

  # plot losses
  df = pd.DataFrame([(int(x), float(y)) for x, _, y in cem_loss_log], columns=['epoch', 'loss'])
  sns.relplot(df, x='epoch', y='loss', kind='line')

  _ = plt.tight_layout()
  _ = plt.show()
#+end_src

#+RESULTS:
:RESULTS:
#+attr_org: :width 484
[[file:./.ob-jupyter/6a99d9467be35764e879e334f59da6c04468e95f.png]]
:END:

** Sampling
We sample images from the trained CEM model.
#+ATTR_LATEX: :options frame=single, linenos, breaklines, tabsize=2
#+begin_src jupyter-python :session py :exports both :async yes :eval never-export
  import matplotlib.pyplot as plt
  import optax
  from jax import random
  import jax.numpy as jnp
  from tqdm import tqdm

  import utils
  from unet import Unet

  def sample(state, n, ts, key):
    # random white noise X_T
    key, subkey = random.split(key)
    x_t = random.normal(subkey, shape=(n, 28, 28, 1))

    step=0

    for k in tqdm(range(len(ts))[::-1]):
      key, subkey = random.split(key)
      z = random.normal(subkey, shape=x_t.shape)

      t = ts[k]
      dt = jnp.where(k > 0, t - ts[k-1], 0.)

      f_theta = state.apply_fn(state.params, x_t, t * jnp.ones((n,)))

      # equation (40)
      s_theta = jnp.where(k > 0, x_t/(1-jnp.exp(-t))  - jnp.exp(-t/2)/(1-jnp.exp(-t)) * f_theta,  0.)

      # equation (24)
      x_t_bar = x_t - dt * s_theta
      x_t = jnp.exp(dt/2) * x_t_bar + jnp.sqrt(1-jnp.exp(-dt)) * z

      x_t = jnp.clip(x_t, -1., 1.) # should we clip ...
      #x_t = normalize_to_neg_one_to_one(x_t) # or scale?

      step=step+1

    return x_t

  key = random.PRNGKey(SEED)

  # use the best params
  file_path, epoch, step, loss = utils.find_latest_pytree(f'{PROJECT_DIR}/cem_params_*.npy')
  cem_state = utils.create_training_state(params_file=f'{PROJECT_DIR}/cem_params_{epoch}_{step}_{loss}.npy')
  print(f'using parameters from epoch {epoch} with loss {loss}')

  ts = utils.exponential_time_schedule(T, K)

  # generate x_0 from noise
  key, subkey = random.split(key)
  x_0_tilde = sample(cem_state, 16, ts, subkey)

  # plot the data
  utils.show_img_grid(utils.unnormalize_image(x_0_tilde))
#+end_src

#+RESULTS:
:RESULTS:
: using parameters from epoch 9 with loss 0.03557
: 100% 1002/1002 [09:41<00:00,  1.72it/s]
:
[[file:./.ob-jupyter/0f3f0e54e530983abb7d269c4c35266ac2b88c7c.png]]
:END:
