
@jit
def forward_process(key, x_0, t, eta):
    # equation 17
    assert x_0.shape[0] == t.shape[0]
    assert len(t.shape) == 1

    n = x_0.shape[0]
    return x_0 * jnp.exp(-t/2).reshape((n, 1, 1, 1)) + eta * jnp.sqrt(1-jnp.exp(-t)).reshape((n, 1, 1, 1))

  def fit(state, time_schedule, key, batch_size, n_epoch, step=0, epoch_start=0):
    @jit
    def lambda_t(t):
      return jnp.where(t>0.0, t/(jnp.exp(t)-1.), jnp.ones(t.shape))

    @jit
    def weighted_mse_loss(params, inputs, time, targets):
      assert inputs.shape[0] == time.shape[0]

      n = targets.shape[0]
      predictions = predict_fn(params, inputs, time)
      weight = lambda_t(time) # (n,)
      diffs = predictions.reshape((n, - 1)) - targets.reshape((n, -1)) # differences of the flattened images (n, 28*28)
      return (weight * (diffs * diffs).mean(axis=1)).mean()

    n_batch=normalized_images.shape[0] // batch_size

    loss_log = None
    best_loss = 1.
    predict_fn = state.apply_fn

    ks = jnp.array(range(len(time_schedule)))

    for epoch in range(epoch_start, n_epoch):
      key, subkey = random.split(key)
      perms = random.permutation(subkey, normalized_images.shape[0])
      perms = perms[: n_batch * batch_size] # skip incomplete batch
      perms = perms.reshape((n_batch, batch_size))

      loss_log = []

      for perm in tqdm(perms, desc=f'epoch {epoch}'):

        # randomly pick a subset of the entire sample size
        x_0_batch = normalized_images[perm, ...]

        # regenerate a new random keys
        key, key2, key3 = random.split(key, 3)

        #noise_level = random.choice(key2, ks, shape=(x_0_batch.shape[0], )) # (n,)
        #t = time_schedule[noise_level]
        t = random.choice(key2, time_schedule, shape=(x_0_batch.shape[0],))

        training_inputs = forward_process(key3, x_0_batch, t)
        training_targets = x_0_batch

        loss, grads = value_and_grad(weighted_mse_loss)(state.params, training_inputs, t, training_targets)

        state = state.apply_gradients(grads=grads)

        step = step+1
        loss_log.append((epoch, step, loss))

        #del x_0_batch
        #del loss

      save_checkpoint(f'{PROJECT_DIR}/cem', state, epoch, step)
      save_loss_log(loss_log, f'{PROJECT_DIR}/mnist_cem_loss_log.npy')

      epoch_loss = np.mean([loss for _, _, loss in loss_log])
      if epoch_loss < best_loss:
        best_loss = epoch_loss
        save_pytree(state.params, f'{PROJECT_DIR}/mnist_cem_params_{epoch}_{step}_{best_loss:.5f}')


      #del loss_log
      #del perms
      #jax.clear_caches()
      #jax.clear_backends()
      #gc.collect()

    return state

if __name__ == 'main':
    key = random.PRNGKey(SEED)
    key, key2, key3 = random.split(key, 3)

  n_epoch=30
  RESUME=False
  epoch_start=-1
  step=-1

  if RESUME:
    state, epoch_start, step = restore_checkpoint(f'{PROJECT_DIR}/cem')
    print(f'restore checkpoint from epoch {epoch_start} and step {step}')
  else:
    state = create_training_state(key2)
    save_checkpoint(f'{PROJECT_DIR}/cem', state, epoch_start, step)

  time_schedule = exponential_time_schedule(0., T, K)[1:] # ignore 0.0

  start = time.time()
  state = fit(state, time_schedule, key3, BATCH_SIZE, n_epoch, step=step+1, epoch_start=epoch_start+1)
  end = time.time()

  print(f'elapsed: {end - start}s')
